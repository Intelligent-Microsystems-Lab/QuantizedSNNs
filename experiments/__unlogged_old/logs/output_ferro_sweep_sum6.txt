Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Traceback (most recent call last):
  File "sweep_sumDVS.py", line 126, in <module>
    train_data = pickle.load(f)
MemoryError
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Traceback (most recent call last):
  File "sweep_sumDVS.py", line 126, in <module>
    train_data = pickle.load(f)
MemoryError
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
hello
hello
hello
hello
hello
hello1
hello1
hello1
Traceback (most recent call last):
  File "sweep_sumDVS.py", line 129, in <module>
    train_data = pickle.load(f)
MemoryError
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
init done
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00011200000000000001, 9.82e-05]}
Epoch 1: loss=312.63865
Test accuracy: 0.297
Train accuracy: 0.199
Epoch 2: loss=51.39209
Test accuracy: 0.262
Train accuracy: 0.328
Epoch 3: loss=51.13516
Test accuracy: 0.301
Train accuracy: 0.346
Epoch 4: loss=50.62601
Test accuracy: 0.309
Train accuracy: 0.396
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00027556290188270857, 0.00024160961575787483]}
Epoch 1: loss=55.37598
Test accuracy: 0.383
Train accuracy: 0.181
Epoch 2: loss=50.96391
Test accuracy: 0.273
Train accuracy: 0.431
Epoch 3: loss=50.13759
Test accuracy: 0.355
Train accuracy: 0.482
Epoch 4: loss=49.79368
Test accuracy: 0.352
Train accuracy: 0.572
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0006779902936966005, 0.0005944522039375551]}
Epoch 1: loss=55.50334
Test accuracy: 0.332
Train accuracy: 0.196
Epoch 2: loss=51.23817
Test accuracy: 0.379
Train accuracy: 0.404
Epoch 3: loss=50.17153
Test accuracy: 0.410
Train accuracy: 0.508
Epoch 4: loss=49.88719
Test accuracy: 0.426
Train accuracy: 0.568
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.001668115828387008, 0.0014625801281036087]}
Epoch 1: loss=55.26054
Test accuracy: 0.266
Train accuracy: 0.166
Epoch 2: loss=50.79121
Test accuracy: 0.336
Train accuracy: 0.405
Epoch 3: loss=50.38430
Test accuracy: 0.320
Train accuracy: 0.420
Epoch 4: loss=50.06588
Test accuracy: 0.348
Train accuracy: 0.473
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0041042039138107335, 0.003598507360144768]}
Epoch 1: loss=54.74228
Test accuracy: 0.297
Train accuracy: 0.216
Epoch 2: loss=50.75335
Test accuracy: 0.340
Train accuracy: 0.425
Epoch 3: loss=50.05441
Test accuracy: 0.371
Train accuracy: 0.503
Epoch 4: loss=49.74811
Test accuracy: 0.398
Train accuracy: 0.563
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.010097913753643354, 0.008853706523283726]}
Traceback (most recent call last):
  File "sweep_sumDVS.py", line 407, in <module>
    loss_hist, test_acc, train_acc, best = train(x_train, y_train, lr = quantization.global_lr, nb_epochs = 4)
  File "sweep_sumDVS.py", line 348, in train
    loss_val.backward()
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 10.73 GiB total capacity; 8.60 GiB already allocated; 129.56 MiB free; 1.08 GiB cached)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00011200000000000001, 9.82e-05]}
Epoch 1: loss=54.40656
Test accuracy: 0.391
Train accuracy: 0.195
Epoch 2: loss=50.85500
Test accuracy: 0.320
Train accuracy: 0.431
Epoch 3: loss=50.01320
Test accuracy: 0.355
Train accuracy: 0.517
Epoch 4: loss=49.70200
Test accuracy: 0.371
Train accuracy: 0.598
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00027556290188270857, 0.00024160961575787483]}
Epoch 1: loss=54.41551
Test accuracy: 0.293
Train accuracy: 0.174
Epoch 2: loss=51.23581
Test accuracy: 0.277
Train accuracy: 0.389
Epoch 3: loss=50.16990
Test accuracy: 0.363
Train accuracy: 0.436
Epoch 4: loss=49.88628
Test accuracy: 0.391
Train accuracy: 0.512
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0006779902936966005, 0.0005944522039375551]}
Epoch 1: loss=55.47705
Test accuracy: 0.297
Train accuracy: 0.208
Epoch 2: loss=51.24386
Test accuracy: 0.312
Train accuracy: 0.398
Epoch 3: loss=50.20289
Test accuracy: 0.297
Train accuracy: 0.456
Epoch 4: loss=49.99017
Test accuracy: 0.328
Train accuracy: 0.485
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.001668115828387008, 0.0014625801281036087]}
Epoch 1: loss=55.61343
Test accuracy: 0.336
Train accuracy: 0.214
Epoch 2: loss=51.51626
Test accuracy: 0.289
Train accuracy: 0.405
Epoch 3: loss=50.26212
Test accuracy: 0.328
Train accuracy: 0.440
Epoch 4: loss=50.00641
Test accuracy: 0.344
Train accuracy: 0.510
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0041042039138107335, 0.003598507360144768]}
Epoch 1: loss=54.49780
Test accuracy: 0.246
Train accuracy: 0.172
Epoch 2: loss=50.47360
Test accuracy: 0.363
Train accuracy: 0.427
Epoch 3: loss=50.08204
Test accuracy: 0.375
Train accuracy: 0.475
Epoch 4: loss=49.83141
Test accuracy: 0.379
Train accuracy: 0.525
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.010097913753643354, 0.008853706523283726]}
Epoch 1: loss=54.23253
Test accuracy: 0.320
Train accuracy: 0.170
Epoch 2: loss=50.54645
Test accuracy: 0.441
Train accuracy: 0.451
Epoch 3: loss=49.91689
Test accuracy: 0.441
Train accuracy: 0.541
Epoch 4: loss=49.62827
Test accuracy: 0.438
Train accuracy: 0.605
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.024844735865314944, 0.021783509481910067]}
Epoch 1: loss=251.20961
Test accuracy: 0.211
Train accuracy: 0.201
Epoch 2: loss=50.61042
Test accuracy: 0.328
Train accuracy: 0.337
Epoch 3: loss=50.16826
Test accuracy: 0.348
Train accuracy: 0.411
Epoch 4: loss=49.92774
Test accuracy: 0.367
Train accuracy: 0.460
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.06112756706746064, 0.05359577755379138]}
Epoch 1: loss=53.23238
Test accuracy: 0.320
Train accuracy: 0.216
Epoch 2: loss=50.15777
Test accuracy: 0.379
Train accuracy: 0.502
Epoch 3: loss=49.63570
Test accuracy: 0.375
Train accuracy: 0.564
Epoch 4: loss=49.37039
Test accuracy: 0.426
Train accuracy: 0.641
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.15039723005481556, 0.13186614278020434]}
Epoch 1: loss=54.60617
Test accuracy: 0.383
Train accuracy: 0.218
Epoch 2: loss=50.37941
Test accuracy: 0.461
Train accuracy: 0.457
Epoch 3: loss=49.38230
Test accuracy: 0.445
Train accuracy: 0.541
Epoch 4: loss=49.07851
Test accuracy: 0.465
Train accuracy: 0.605
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.37003479597344896, 0.324441222898149]}
Epoch 1: loss=53.46963
Test accuracy: 0.375
Train accuracy: 0.178
Epoch 2: loss=49.33378
Test accuracy: 0.293
Train accuracy: 0.422
Epoch 3: loss=48.64532
Test accuracy: 0.375
Train accuracy: 0.460
Epoch 4: loss=48.31973
Test accuracy: 0.410
Train accuracy: 0.551
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.9104273408573178, 0.7982496863588268]}
Epoch 1: loss=53.07606
Test accuracy: 0.324
Train accuracy: 0.187
Epoch 2: loss=47.41037
Test accuracy: 0.406
Train accuracy: 0.424
Epoch 3: loss=46.47326
Test accuracy: 0.359
Train accuracy: 0.510
Epoch 4: loss=46.17165
Test accuracy: 0.391
Train accuracy: 0.570
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.24, 1.964]}
Epoch 1: loss=47.94523
Test accuracy: 0.332
Train accuracy: 0.182
Epoch 2: loss=42.30709
Test accuracy: 0.320
Train accuracy: 0.366
Epoch 3: loss=41.57579
Test accuracy: 0.328
Train accuracy: 0.413
Epoch 4: loss=41.27558
Test accuracy: 0.336
Train accuracy: 0.438

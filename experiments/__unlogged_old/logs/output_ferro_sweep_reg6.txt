Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
python: can't open file 'sweep_regDVS.py': [Errno 2] No such file or directory
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Traceback (most recent call last):
  File "swep_regDVS.py", line 126, in <module>
    train_data = pickle.load(f)
MemoryError
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
hello
hello
hello
hello
hello
hello
hello1
hello1
hello1
hello1
hello1
init done
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=3.89999
Test accuracy: 0.289
Train accuracy: 0.263
Epoch 2: loss=1.79619
Test accuracy: 0.348
Train accuracy: 0.437
Epoch 3: loss=1.47309
Test accuracy: 0.406
Train accuracy: 0.565
Epoch 4: loss=1.20895
Test accuracy: 0.371
Train accuracy: 0.655
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=2.99099
Test accuracy: 0.344
Train accuracy: 0.339
Epoch 2: loss=1.48759
Test accuracy: 0.398
Train accuracy: 0.569
Epoch 3: loss=1.19143
Test accuracy: 0.422
Train accuracy: 0.675
Epoch 4: loss=0.93786
Test accuracy: 0.480
Train accuracy: 0.770
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=13.30465
Test accuracy: 0.098
Train accuracy: 0.162
Epoch 2: loss=6.22828
Test accuracy: 0.109
Train accuracy: 0.158
Epoch 3: loss=3.65861
Test accuracy: 0.105
Train accuracy: 0.185
Epoch 4: loss=2.45474
Test accuracy: 0.121
Train accuracy: 0.246
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=4.51326
Test accuracy: 0.348
Train accuracy: 0.332
Epoch 2: loss=2.36740
Test accuracy: 0.379
Train accuracy: 0.510
Epoch 3: loss=1.90844
Test accuracy: 0.402
Train accuracy: 0.616
Epoch 4: loss=1.65737
Test accuracy: 0.434
Train accuracy: 0.703
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=5.75423
Test accuracy: 0.316
Train accuracy: 0.324
Epoch 2: loss=3.64724
Test accuracy: 0.344
Train accuracy: 0.550
Epoch 3: loss=3.24999
Test accuracy: 0.426
Train accuracy: 0.658
Epoch 4: loss=3.01101
Test accuracy: 0.441
Train accuracy: 0.773
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=14.53670
Test accuracy: 0.176
Train accuracy: 0.229
Epoch 2: loss=11.03854
Test accuracy: 0.227
Train accuracy: 0.293
Epoch 3: loss=9.47355
Test accuracy: 0.273
Train accuracy: 0.382
Epoch 4: loss=8.67932
Test accuracy: 0.305
Train accuracy: 0.497
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=27.23088
Test accuracy: 0.426
Train accuracy: 0.321
Epoch 2: loss=24.81898
Test accuracy: 0.344
Train accuracy: 0.589
Epoch 3: loss=23.06268
Test accuracy: 0.453
Train accuracy: 0.704
Epoch 4: loss=21.38105
Test accuracy: 0.465
Train accuracy: 0.804
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=92.90113
Test accuracy: 0.133
Train accuracy: 0.195
Epoch 2: loss=83.25525
Test accuracy: 0.219
Train accuracy: 0.191
Epoch 3: loss=76.30775
Test accuracy: 0.297
Train accuracy: 0.337
Epoch 4: loss=70.71325
Test accuracy: 0.336
Train accuracy: 0.460
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=293.68345
Test accuracy: 0.180
Train accuracy: 0.170
Epoch 2: loss=271.69477
Test accuracy: 0.316
Train accuracy: 0.360
Epoch 3: loss=250.89596
Test accuracy: 0.340
Train accuracy: 0.472
Epoch 4: loss=230.41645
Test accuracy: 0.383
Train accuracy: 0.531
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=930.99985
Test accuracy: 0.324
Train accuracy: 0.323
Epoch 2: loss=721.46149
Test accuracy: 0.410
Train accuracy: 0.518
Epoch 3: loss=534.49768
Test accuracy: 0.395
Train accuracy: 0.582
Epoch 4: loss=376.05596
Test accuracy: 0.426
Train accuracy: 0.661
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=3151.84736
Test accuracy: 0.340
Train accuracy: 0.358
Epoch 2: loss=2377.74948
Test accuracy: 0.391
Train accuracy: 0.488
Epoch 3: loss=1709.96164
Test accuracy: 0.391
Train accuracy: 0.572
Epoch 4: loss=1160.53059
Test accuracy: 0.391
Train accuracy: 0.635
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=10739.18934
Test accuracy: 0.230
Train accuracy: 0.226
Epoch 2: loss=7956.91526
Test accuracy: 0.367
Train accuracy: 0.443
Epoch 3: loss=5621.35493
Test accuracy: 0.375
Train accuracy: 0.477
Epoch 4: loss=3732.29647
Test accuracy: 0.363
Train accuracy: 0.519
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
init done
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=17.69120
Test accuracy: 0.152
Train accuracy: 0.173
Epoch 2: loss=4.66764
Test accuracy: 0.219
Train accuracy: 0.172
Epoch 3: loss=3.58678
Test accuracy: 0.246
Train accuracy: 0.241
Epoch 4: loss=2.84440
Test accuracy: 0.277
Train accuracy: 0.289
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=12.91373
Test accuracy: 0.156
Train accuracy: 0.139
Epoch 2: loss=4.34457
Test accuracy: 0.246
Train accuracy: 0.163
Epoch 3: loss=3.11701
Test accuracy: 0.262
Train accuracy: 0.241
Epoch 4: loss=2.49541
Test accuracy: 0.309
Train accuracy: 0.286
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=8.34749
Test accuracy: 0.215
Train accuracy: 0.202
Epoch 2: loss=2.83184
Test accuracy: 0.227
Train accuracy: 0.362
Epoch 3: loss=2.36315
Test accuracy: 0.320
Train accuracy: 0.414
Epoch 4: loss=2.09337
Test accuracy: 0.316
Train accuracy: 0.478
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=8.15532
Test accuracy: 0.328
Train accuracy: 0.188
Epoch 2: loss=4.13186
Test accuracy: 0.332
Train accuracy: 0.409
Epoch 3: loss=3.40105
Test accuracy: 0.348
Train accuracy: 0.477
Epoch 4: loss=3.17124
Test accuracy: 0.332
Train accuracy: 0.557
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=15.75246
Test accuracy: 0.293
Train accuracy: 0.169
Epoch 2: loss=9.03123
Test accuracy: 0.234
Train accuracy: 0.329
Epoch 3: loss=8.47412
Test accuracy: 0.293
Train accuracy: 0.367
Epoch 4: loss=8.16383
Test accuracy: 0.266
Train accuracy: 0.398
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=27.81131
Test accuracy: 0.305
Train accuracy: 0.166
Epoch 2: loss=23.64454
Test accuracy: 0.387
Train accuracy: 0.394
Epoch 3: loss=22.92830
Test accuracy: 0.410
Train accuracy: 0.445
Epoch 4: loss=22.61109
Test accuracy: 0.422
Train accuracy: 0.546
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Traceback (most recent call last):
  File "swep_regDVS.py", line 403, in <module>
    loss_hist, test_acc, train_acc, best = train(x_train, y_train, lr = quantization.global_lr, nb_epochs = 4)
  File "swep_regDVS.py", line 348, in train
    loss_val.backward()
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 11.75 GiB total capacity; 9.33 GiB already allocated; 73.75 MiB free; 1.21 GiB cached)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=6.91271
Test accuracy: 0.266
Train accuracy: 0.185
Epoch 2: loss=2.47247
Test accuracy: 0.328
Train accuracy: 0.312
Epoch 3: loss=1.72563
Test accuracy: 0.375
Train accuracy: 0.428
Epoch 4: loss=1.58682
Test accuracy: 0.363
Train accuracy: 0.474
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=6.54954
Test accuracy: 0.328
Train accuracy: 0.194
Epoch 2: loss=2.21500
Test accuracy: 0.363
Train accuracy: 0.418
Epoch 3: loss=1.73994
Test accuracy: 0.379
Train accuracy: 0.463
Epoch 4: loss=1.53446
Test accuracy: 0.379
Train accuracy: 0.542
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=6.97094
Test accuracy: 0.324
Train accuracy: 0.191
Epoch 2: loss=2.73976
Test accuracy: 0.301
Train accuracy: 0.381
Epoch 3: loss=2.11391
Test accuracy: 0.340
Train accuracy: 0.469
Epoch 4: loss=1.89782
Test accuracy: 0.352
Train accuracy: 0.530
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=185.39946
Test accuracy: 0.289
Train accuracy: 0.193
Epoch 2: loss=4.08794
Test accuracy: 0.191
Train accuracy: 0.344
Epoch 3: loss=4.40683
Test accuracy: 0.207
Train accuracy: 0.262
Epoch 4: loss=3.91474
Test accuracy: 0.266
Train accuracy: 0.341
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=13.54367
Test accuracy: 0.312
Train accuracy: 0.218
Epoch 2: loss=8.57593
Test accuracy: 0.324
Train accuracy: 0.413
Epoch 3: loss=8.02659
Test accuracy: 0.340
Train accuracy: 0.453
Epoch 4: loss=7.85073
Test accuracy: 0.328
Train accuracy: 0.484
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=28.26802
Test accuracy: 0.324
Train accuracy: 0.200
Epoch 2: loss=23.41289
Test accuracy: 0.359
Train accuracy: 0.405
Epoch 3: loss=22.94751
Test accuracy: 0.367
Train accuracy: 0.458
Epoch 4: loss=22.74698
Test accuracy: 0.414
Train accuracy: 0.493
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=79.30661
Test accuracy: 0.352
Train accuracy: 0.194
Epoch 2: loss=75.42568
Test accuracy: 0.312
Train accuracy: 0.411
Epoch 3: loss=74.64020
Test accuracy: 0.395
Train accuracy: 0.458
Epoch 4: loss=74.23611
Test accuracy: 0.414
Train accuracy: 0.519
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=267.05732
Test accuracy: 0.266
Train accuracy: 0.174
Epoch 2: loss=253.23305
Test accuracy: 0.289
Train accuracy: 0.343
Epoch 3: loss=251.68473
Test accuracy: 0.293
Train accuracy: 0.424
Epoch 4: loss=250.33679
Test accuracy: 0.320
Train accuracy: 0.444
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=872.09645
Test accuracy: 0.316
Train accuracy: 0.194
Epoch 2: loss=860.72147
Test accuracy: 0.332
Train accuracy: 0.388
Epoch 3: loss=852.55938
Test accuracy: 0.395
Train accuracy: 0.479
Epoch 4: loss=844.36052
Test accuracy: 0.410
Train accuracy: 0.529
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=2972.37463
Test accuracy: 0.336
Train accuracy: 0.199
Epoch 2: loss=2926.59859
Test accuracy: 0.363
Train accuracy: 0.411
Epoch 3: loss=2880.44430
Test accuracy: 0.410
Train accuracy: 0.502
Epoch 4: loss=2833.74158
Test accuracy: 0.395
Train accuracy: 0.549
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=10195.93283
Test accuracy: 0.262
Train accuracy: 0.183
Epoch 2: loss=10017.51693
Test accuracy: 0.324
Train accuracy: 0.387
Epoch 3: loss=9843.74577
Test accuracy: 0.344
Train accuracy: 0.406
Epoch 4: loss=9669.82476
Test accuracy: 0.332
Train accuracy: 0.469
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=35046.07595
Test accuracy: 0.266
Train accuracy: 0.187
Epoch 2: loss=34391.95269
Test accuracy: 0.324
Train accuracy: 0.343
Epoch 3: loss=33748.70095
Test accuracy: 0.387
Train accuracy: 0.411
Epoch 4: loss=33114.74718
Test accuracy: 0.383
Train accuracy: 0.420
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=3.51610
Test accuracy: 0.324
Train accuracy: 0.181
Epoch 2: loss=1.96939
Test accuracy: 0.285
Train accuracy: 0.366
Epoch 3: loss=1.69548
Test accuracy: 0.309
Train accuracy: 0.396
Epoch 4: loss=1.55966
Test accuracy: 0.328
Train accuracy: 0.458
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=3.60244
Test accuracy: 0.312
Train accuracy: 0.194
Epoch 2: loss=1.91751
Test accuracy: 0.320
Train accuracy: 0.378
Epoch 3: loss=1.69135
Test accuracy: 0.352
Train accuracy: 0.440
Epoch 4: loss=1.54578
Test accuracy: 0.383
Train accuracy: 0.506
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=3.81140
Test accuracy: 0.301
Train accuracy: 0.190
Epoch 2: loss=2.14733
Test accuracy: 0.312
Train accuracy: 0.415
Epoch 3: loss=1.89195
Test accuracy: 0.324
Train accuracy: 0.434
Epoch 4: loss=1.78378
Test accuracy: 0.348
Train accuracy: 0.470
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=4.75237
Test accuracy: 0.297
Train accuracy: 0.173
Epoch 2: loss=3.10655
Test accuracy: 0.312
Train accuracy: 0.345
Epoch 3: loss=2.85146
Test accuracy: 0.348
Train accuracy: 0.400
Epoch 4: loss=2.72083
Test accuracy: 0.391
Train accuracy: 0.485
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=7.61794
Test accuracy: 0.340
Train accuracy: 0.201
Epoch 2: loss=6.07128
Test accuracy: 0.277
Train accuracy: 0.375
Epoch 3: loss=5.86725
Test accuracy: 0.348
Train accuracy: 0.416
Epoch 4: loss=5.74546
Test accuracy: 0.387
Train accuracy: 0.469
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=18.08664
Test accuracy: 0.273
Train accuracy: 0.177
Epoch 2: loss=16.45923
Test accuracy: 0.344
Train accuracy: 0.366
Epoch 3: loss=16.18814
Test accuracy: 0.371
Train accuracy: 0.424
Epoch 4: loss=16.03203
Test accuracy: 0.402
Train accuracy: 0.484
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=53.24849
Test accuracy: 0.293
Train accuracy: 0.179
Epoch 2: loss=51.90485
Test accuracy: 0.355
Train accuracy: 0.422
Epoch 3: loss=51.48274
Test accuracy: 0.371
Train accuracy: 0.485
Epoch 4: loss=51.12781
Test accuracy: 0.387
Train accuracy: 0.562
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=176.42817
Test accuracy: 0.309
Train accuracy: 0.195
Epoch 2: loss=173.70205
Test accuracy: 0.398
Train accuracy: 0.416
Epoch 3: loss=172.13216
Test accuracy: 0.410
Train accuracy: 0.477
Epoch 4: loss=170.55343
Test accuracy: 0.430
Train accuracy: 0.510
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=598.54372
Test accuracy: 0.250
Train accuracy: 0.160
Epoch 2: loss=589.91372
Test accuracy: 0.312
Train accuracy: 0.326
Epoch 3: loss=582.24303
Test accuracy: 0.371
Train accuracy: 0.387
Epoch 4: loss=574.42666
Test accuracy: 0.391
Train accuracy: 0.453
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=2045.37441
Test accuracy: 0.297
Train accuracy: 0.194
Epoch 2: loss=2011.79938
Test accuracy: 0.359
Train accuracy: 0.385
Epoch 3: loss=1979.03406
Test accuracy: 0.324
Train accuracy: 0.442
Epoch 4: loss=1946.52890
Test accuracy: 0.367
Train accuracy: 0.481
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=7025.01308
Test accuracy: 0.297
Train accuracy: 0.190
Epoch 2: loss=6905.98836
Test accuracy: 0.324
Train accuracy: 0.336
Epoch 3: loss=6789.40332
Test accuracy: 0.391
Train accuracy: 0.385
Epoch 4: loss=6674.45521
Test accuracy: 0.418
Train accuracy: 0.451
{'quantization.global_wb': 6, 'inp_mult': 120, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=24160.29861
Test accuracy: 0.145
Train accuracy: 0.148
Epoch 2: loss=23749.53136
Test accuracy: 0.230
Train accuracy: 0.240
Epoch 3: loss=23345.59744
Test accuracy: 0.281
Train accuracy: 0.332
Epoch 4: loss=22948.10916
Test accuracy: 0.297
Train accuracy: 0.339

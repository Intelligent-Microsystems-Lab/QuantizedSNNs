Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
python: can't open file 'sweep_regDVS.py': [Errno 2] No such file or directory
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
hello
hello
hello
hello
hello
hello
hello1
hello1
hello1
Traceback (most recent call last):
  File "swep_regDVS.py", line 129, in <module>
    train_data = pickle.load(f)
MemoryError
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
init done
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=10.00491
Test accuracy: 0.145
Train accuracy: 0.135
Epoch 2: loss=245989.32516
Test accuracy: 0.207
Train accuracy: 0.262
Epoch 3: loss=37184.16508
Test accuracy: 0.180
Train accuracy: 0.248
Epoch 4: loss=899.07348
Test accuracy: 0.168
Train accuracy: 0.232
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=364.36532
Test accuracy: 0.062
Train accuracy: 0.069
Epoch 2: loss=41360.56662
Test accuracy: 0.102
Train accuracy: 0.099
Epoch 3: loss=1834227233176216122949632.00000
Test accuracy: 0.055
Train accuracy: 0.097
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=7.94817
Test accuracy: 0.223
Train accuracy: 0.220
Epoch 2: loss=155.36031
Test accuracy: 0.215
Train accuracy: 0.370
Epoch 3: loss=2182.69574
Test accuracy: 0.160
Train accuracy: 0.371
Epoch 4: loss=276738944.31050
Test accuracy: 0.211
Train accuracy: 0.382
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=7151010566135238656.00000
Test accuracy: 0.148
Train accuracy: 0.115
Epoch 2: loss=2246439571568.28418
Test accuracy: 0.156
Train accuracy: 0.157
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=617.16629
Test accuracy: 0.160
Train accuracy: 0.100
Epoch 2: loss=20.72176
Test accuracy: 0.152
Train accuracy: 0.135
Epoch 3: loss=170094477.92566
Test accuracy: 0.137
Train accuracy: 0.144
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Traceback (most recent call last):
  File "swep_regDVS.py", line 403, in <module>
    loss_hist, test_acc, train_acc, best = train(x_train, y_train, lr = quantization.global_lr, nb_epochs = 4)
  File "swep_regDVS.py", line 348, in train
    loss_val.backward()
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 10.73 GiB total capacity; 8.60 GiB already allocated; 127.56 MiB free; 1.09 GiB cached)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=16.50833
Test accuracy: 0.074
Train accuracy: 0.101
Epoch 2: loss=27.98613
Test accuracy: 0.121
Train accuracy: 0.154
Epoch 3: loss=340767735.54372
Test accuracy: 0.145
Train accuracy: 0.212
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=12.22762
Test accuracy: 0.223
Train accuracy: 0.207
Epoch 2: loss=13.62153
Test accuracy: 0.223
Train accuracy: 0.308
Epoch 3: loss=16.09027
Test accuracy: 0.273
Train accuracy: 0.391
Epoch 4: loss=137228460.99997
Test accuracy: 0.242
Train accuracy: 0.468
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=239.98261
Test accuracy: 0.105
Train accuracy: 0.089
Epoch 2: loss=215448719011193061376.00000
Test accuracy: 0.102
Train accuracy: 0.168
Epoch 3: loss=500.12897
Test accuracy: 0.145
Train accuracy: 0.211
Epoch 4: loss=1627653.88249
Test accuracy: 0.129
Train accuracy: 0.245
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=4503.07175
Test accuracy: 0.066
Train accuracy: 0.106
Epoch 2: loss=7096157.63466
Test accuracy: 0.078
Train accuracy: 0.089
Epoch 3: loss=12566582.74619
Test accuracy: 0.055
Train accuracy: 0.086
Epoch 4: loss=1540.37749
Test accuracy: 0.066
Train accuracy: 0.074
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=105553274892.47838
Test accuracy: 0.168
Train accuracy: 0.190
Epoch 2: loss=2425575970508.58350
Test accuracy: 0.125
Train accuracy: 0.144
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=4273540.31848
Test accuracy: 0.113
Train accuracy: 0.094
Epoch 2: loss=960266.39526
Test accuracy: 0.168
Train accuracy: 0.217
Epoch 3: loss=4.42845
Test accuracy: 0.191
Train accuracy: 0.249
Epoch 4: loss=3.88034
Test accuracy: 0.188
Train accuracy: 0.275
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=1994.66436
Test accuracy: 0.133
Train accuracy: 0.113
Epoch 2: loss=39215.57151
Test accuracy: 0.117
Train accuracy: 0.162
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=15.06946
Test accuracy: 0.129
Train accuracy: 0.133
Epoch 2: loss=590188302538761.50000
Test accuracy: 0.152
Train accuracy: 0.247
Epoch 3: loss=219521114625138720768.00000
Test accuracy: 0.137
Train accuracy: 0.244
Epoch 4: loss=988356.87359
Test accuracy: 0.113
Train accuracy: 0.228
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=1246479.51158
Test accuracy: 0.133
Train accuracy: 0.115
Epoch 2: loss=395727013.01393
Test accuracy: 0.070
Train accuracy: 0.169
Epoch 3: loss=70753279694.36610
Test accuracy: 0.059
Train accuracy: 0.103
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=3131255088149.74121
Test accuracy: 0.176
Train accuracy: 0.128
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=4594.18504
Test accuracy: 0.098
Train accuracy: 0.115
Epoch 2: loss=16.61210
Test accuracy: 0.113
Train accuracy: 0.122
Epoch 3: loss=10.60271
Test accuracy: 0.129
Train accuracy: 0.133
Epoch 4: loss=9.52420
Test accuracy: 0.117
Train accuracy: 0.154
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=50.89933
Test accuracy: 0.176
Train accuracy: 0.143
Epoch 2: loss=42.80258
Test accuracy: 0.191
Train accuracy: 0.204
Epoch 3: loss=7.13666
Test accuracy: 0.188
Train accuracy: 0.234
Epoch 4: loss=11.56501
Test accuracy: 0.199
Train accuracy: 0.276
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=5.52482
Test accuracy: 0.207
Train accuracy: 0.138
Epoch 2: loss=2.17068
Test accuracy: 0.211
Train accuracy: 0.289
Epoch 3: loss=1.81336
Test accuracy: 0.234
Train accuracy: 0.406
Epoch 4: loss=1.58030
Test accuracy: 0.238
Train accuracy: 0.487
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=40.51997
Test accuracy: 0.145
Train accuracy: 0.139
Epoch 2: loss=3.76179
Test accuracy: 0.148
Train accuracy: 0.227
Epoch 3: loss=3.02251
Test accuracy: 0.160
Train accuracy: 0.268
Epoch 4: loss=2.64657
Test accuracy: 0.176
Train accuracy: 0.306
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=10581.64354
Test accuracy: 0.066
Train accuracy: 0.080
Epoch 2: loss=9.18993
Test accuracy: 0.070
Train accuracy: 0.089
Epoch 3: loss=7.86052
Test accuracy: 0.070
Train accuracy: 0.100
Epoch 4: loss=32.02150
Test accuracy: 0.070
Train accuracy: 0.109
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=4.92714
Test accuracy: 0.258
Train accuracy: 0.195
Epoch 2: loss=2.12652
Test accuracy: 0.238
Train accuracy: 0.372
Epoch 3: loss=1.75298
Test accuracy: 0.242
Train accuracy: 0.476
Epoch 4: loss=1.49460
Test accuracy: 0.230
Train accuracy: 0.567
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=69.05763
Test accuracy: 0.125
Train accuracy: 0.119
Epoch 2: loss=2.97060
Test accuracy: 0.168
Train accuracy: 0.220
Epoch 3: loss=2.48642
Test accuracy: 0.188
Train accuracy: 0.312
Epoch 4: loss=2.20494
Test accuracy: 0.195
Train accuracy: 0.418
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=6.33737
Test accuracy: 0.199
Train accuracy: 0.146
Epoch 2: loss=3.30285
Test accuracy: 0.238
Train accuracy: 0.360
Epoch 3: loss=2.91200
Test accuracy: 0.270
Train accuracy: 0.488
Epoch 4: loss=2.62790
Test accuracy: 0.285
Train accuracy: 0.609
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=551.10049
Test accuracy: 0.152
Train accuracy: 0.112
Epoch 2: loss=9.98658
Test accuracy: 0.152
Train accuracy: 0.133
Epoch 3: loss=8.91844
Test accuracy: 0.168
Train accuracy: 0.161
Epoch 4: loss=8.45125
Test accuracy: 0.160
Train accuracy: 0.188
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=21.62988
Test accuracy: 0.176
Train accuracy: 0.173
Epoch 2: loss=17.85260
Test accuracy: 0.211
Train accuracy: 0.283
Epoch 3: loss=2505.54663
Test accuracy: 0.242
Train accuracy: 0.384
Epoch 4: loss=982548.00427
Test accuracy: 0.246
Train accuracy: 0.437
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=62.39752
Test accuracy: 0.086
Train accuracy: 0.106
Epoch 2: loss=56.24535
Test accuracy: 0.156
Train accuracy: 0.150
Epoch 3: loss=54.46916
Test accuracy: 0.176
Train accuracy: 0.230
Epoch 4: loss=52.91074
Test accuracy: 0.191
Train accuracy: 0.296
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=280.41763
Test accuracy: 0.133
Train accuracy: 0.143
Epoch 2: loss=118641.62411
Test accuracy: 0.145
Train accuracy: 0.114
Epoch 3: loss=153371493.05209
Test accuracy: 0.137
Train accuracy: 0.164
Epoch 4: loss=989.80820
Test accuracy: 0.109
Train accuracy: 0.101
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=641.91851
Test accuracy: 0.129
Train accuracy: 0.153
Epoch 2: loss=608.19665
Test accuracy: 0.172
Train accuracy: 0.252
Epoch 3: loss=577.45170
Test accuracy: 0.180
Train accuracy: 0.294
Epoch 4: loss=547.87960
Test accuracy: 0.223
Train accuracy: 0.319
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=2190.15108
Test accuracy: 0.145
Train accuracy: 0.119
Epoch 2: loss=2075.58185
Test accuracy: 0.180
Train accuracy: 0.218
Epoch 3: loss=1967.49137
Test accuracy: 0.176
Train accuracy: 0.256
Epoch 4: loss=1864.05735
Test accuracy: 0.211
Train accuracy: 0.271

Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
python: can't open file 'sweep_regDVS.py': [Errno 2] No such file or directory
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
init done
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=7.55503
Test accuracy: 0.070
Train accuracy: 0.085
Epoch 2: loss=7.55397
Test accuracy: 0.082
Train accuracy: 0.082
Epoch 3: loss=7.55143
Test accuracy: 0.082
Train accuracy: 0.085
Epoch 4: loss=7.54213
Test accuracy: 0.078
Train accuracy: 0.083
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=15.05618
Test accuracy: 0.086
Train accuracy: 0.083
Epoch 2: loss=15.05847
Test accuracy: 0.090
Train accuracy: 0.082
Epoch 3: loss=15.05087
Test accuracy: 0.074
Train accuracy: 0.084
Epoch 4: loss=15.04181
Test accuracy: 0.082
Train accuracy: 0.083
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=40.80482
Test accuracy: 0.070
Train accuracy: 0.084
Epoch 2: loss=40.80245
Test accuracy: 0.082
Train accuracy: 0.083
Epoch 3: loss=40.79810
Test accuracy: 0.086
Train accuracy: 0.083
Epoch 4: loss=40.79212
Test accuracy: 0.082
Train accuracy: 0.082
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=129.44100
Test accuracy: 0.082
Train accuracy: 0.083
Epoch 2: loss=129.42266
Test accuracy: 0.074
Train accuracy: 0.084
Epoch 3: loss=129.40237
Test accuracy: 0.082
Train accuracy: 0.086
Epoch 4: loss=129.37515
Test accuracy: 0.082
Train accuracy: 0.084
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=434.35200
Test accuracy: 0.086
Train accuracy: 0.082
Epoch 2: loss=434.18187
Test accuracy: 0.090
Train accuracy: 0.082
Epoch 3: loss=434.00905
Test accuracy: 0.082
Train accuracy: 0.080
Epoch 4: loss=433.82055
Test accuracy: 0.094
Train accuracy: 0.091
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Traceback (most recent call last):
  File "swep_regDVS.py", line 403, in <module>
    loss_hist, test_acc, train_acc, best = train(x_train, y_train, lr = quantization.global_lr, nb_epochs = 4)
  File "swep_regDVS.py", line 348, in train
    loss_val.backward()
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 10.73 GiB total capacity; 8.60 GiB already allocated; 173.56 MiB free; 1.03 GiB cached)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=7.59707
Test accuracy: 0.078
Train accuracy: 0.080
Epoch 2: loss=7.59510
Test accuracy: 0.086
Train accuracy: 0.082
Epoch 3: loss=7.58622
Test accuracy: 0.086
Train accuracy: 0.083
Epoch 4: loss=7.58000
Test accuracy: 0.078
Train accuracy: 0.084
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=15.13973
Test accuracy: 0.086
Train accuracy: 0.084
Epoch 2: loss=15.13736
Test accuracy: 0.082
Train accuracy: 0.083
Epoch 3: loss=15.13305
Test accuracy: 0.078
Train accuracy: 0.083
Epoch 4: loss=15.12058
Test accuracy: 0.074
Train accuracy: 0.082
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=40.80160
Test accuracy: 0.082
Train accuracy: 0.084
Epoch 2: loss=40.80164
Test accuracy: 0.078
Train accuracy: 0.082
Epoch 3: loss=40.79554
Test accuracy: 0.078
Train accuracy: 0.082
Epoch 4: loss=40.78338
Test accuracy: 0.086
Train accuracy: 0.082
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=129.45916
Test accuracy: 0.086
Train accuracy: 0.085
Epoch 2: loss=129.44572
Test accuracy: 0.082
Train accuracy: 0.082
Epoch 3: loss=129.42150
Test accuracy: 0.094
Train accuracy: 0.083
Epoch 4: loss=129.39514
Test accuracy: 0.082
Train accuracy: 0.083
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=434.48632
Test accuracy: 0.082
Train accuracy: 0.084
Epoch 2: loss=434.31057
Test accuracy: 0.082
Train accuracy: 0.084
Epoch 3: loss=434.13382
Test accuracy: 0.078
Train accuracy: 0.085
Epoch 4: loss=433.95255
Test accuracy: 0.074
Train accuracy: 0.084
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=1483.46312
Test accuracy: 0.074
Train accuracy: 0.081
Epoch 2: loss=1482.17159
Test accuracy: 0.086
Train accuracy: 0.083
Epoch 3: loss=1480.88879
Test accuracy: 0.082
Train accuracy: 0.082
Epoch 4: loss=1479.60343
Test accuracy: 0.074
Train accuracy: 0.085
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=5091.65763
Test accuracy: 0.086
Train accuracy: 0.082
Epoch 2: loss=5084.19287
Test accuracy: 0.094
Train accuracy: 0.081
Epoch 3: loss=5076.73684
Test accuracy: 0.078
Train accuracy: 0.082
Epoch 4: loss=5069.27488
Test accuracy: 0.078
Train accuracy: 0.083
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=17506.60634
Test accuracy: 0.086
Train accuracy: 0.083
Epoch 2: loss=17472.43913
Test accuracy: 0.090
Train accuracy: 0.081
Epoch 3: loss=17438.32118
Test accuracy: 0.082
Train accuracy: 0.082
Epoch 4: loss=17404.24805
Test accuracy: 0.078
Train accuracy: 0.082
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=60208.87934
Test accuracy: 0.086
Train accuracy: 0.084
Epoch 2: loss=60080.35764
Test accuracy: 0.082
Train accuracy: 0.082
Epoch 3: loss=59952.10417
Test accuracy: 0.078
Train accuracy: 0.082
Epoch 4: loss=59824.09136
Test accuracy: 0.086
Train accuracy: 0.083
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=207120.81337
Test accuracy: 0.086
Train accuracy: 0.085
Epoch 2: loss=206673.79514
Test accuracy: 0.074
Train accuracy: 0.084
Epoch 3: loss=206227.61372
Test accuracy: 0.078
Train accuracy: 0.085
Epoch 4: loss=205782.33333
Test accuracy: 0.082
Train accuracy: 0.082
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=712638.51042
Test accuracy: 0.086
Train accuracy: 0.083
Epoch 2: loss=711098.65278
Test accuracy: 0.090
Train accuracy: 0.083
Epoch 3: loss=709561.65625
Test accuracy: 0.070
Train accuracy: 0.084
Epoch 4: loss=708027.62500
Test accuracy: 0.082
Train accuracy: 0.082
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=2451723.18056
Test accuracy: 0.078
Train accuracy: 0.084
Epoch 2: loss=2446425.11111
Test accuracy: 0.074
Train accuracy: 0.082
Epoch 3: loss=2441136.27778
Test accuracy: 0.094
Train accuracy: 0.083
Epoch 4: loss=2435857.36111
Test accuracy: 0.090
Train accuracy: 0.084
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=4.33821
Test accuracy: 0.082
Train accuracy: 0.083
Epoch 2: loss=4.33850
Test accuracy: 0.090
Train accuracy: 0.083
Epoch 3: loss=4.33740
Test accuracy: 0.090
Train accuracy: 0.083
Epoch 4: loss=4.33655
Test accuracy: 0.078
Train accuracy: 0.082
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=9.01602
Test accuracy: 0.082
Train accuracy: 0.083
Epoch 2: loss=9.01631
Test accuracy: 0.074
Train accuracy: 0.083
Epoch 3: loss=9.01536
Test accuracy: 0.094
Train accuracy: 0.082
Epoch 4: loss=9.01383
Test accuracy: 0.094
Train accuracy: 0.082
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=25.18356
Test accuracy: 0.082
Train accuracy: 0.084
Epoch 2: loss=25.18092
Test accuracy: 0.074
Train accuracy: 0.083
Epoch 3: loss=25.17658
Test accuracy: 0.078
Train accuracy: 0.081
Epoch 4: loss=25.17130
Test accuracy: 0.086
Train accuracy: 0.088
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=80.66778
Test accuracy: 0.078
Train accuracy: 0.082
Epoch 2: loss=80.63830
Test accuracy: 0.082
Train accuracy: 0.082
Epoch 3: loss=80.60679
Test accuracy: 0.090
Train accuracy: 0.084
Epoch 4: loss=80.57614
Test accuracy: 0.082
Train accuracy: 0.085
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=271.66606
Test accuracy: 0.078
Train accuracy: 0.083
Epoch 2: loss=271.43758
Test accuracy: 0.086
Train accuracy: 0.082
Epoch 3: loss=271.21161
Test accuracy: 0.082
Train accuracy: 0.082
Epoch 4: loss=270.98311
Test accuracy: 0.090
Train accuracy: 0.083
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=928.40804
Test accuracy: 0.082
Train accuracy: 0.084
Epoch 2: loss=927.06372
Test accuracy: 0.074
Train accuracy: 0.083
Epoch 3: loss=925.72458
Test accuracy: 0.082
Train accuracy: 0.082
Epoch 4: loss=924.38625
Test accuracy: 0.082
Train accuracy: 0.083
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=3188.50563
Test accuracy: 0.074
Train accuracy: 0.083
Epoch 2: loss=3182.26019
Test accuracy: 0.082
Train accuracy: 0.084
Epoch 3: loss=3176.03010
Test accuracy: 0.082
Train accuracy: 0.082
Epoch 4: loss=3169.80416
Test accuracy: 0.168
Train accuracy: 0.097
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=10960.78342
Test accuracy: 0.180
Train accuracy: 0.160
Epoch 2: loss=10937.42763
Test accuracy: 0.160
Train accuracy: 0.157
Epoch 3: loss=10914.12354
Test accuracy: 0.168
Train accuracy: 0.164
Epoch 4: loss=10890.86930
Test accuracy: 0.168
Train accuracy: 0.168
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=37705.95399
Test accuracy: 0.086
Train accuracy: 0.085
Epoch 2: loss=37624.74523
Test accuracy: 0.090
Train accuracy: 0.082
Epoch 3: loss=37543.69162
Test accuracy: 0.082
Train accuracy: 0.084
Epoch 4: loss=37462.80122
Test accuracy: 0.082
Train accuracy: 0.084
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=129732.81076
Test accuracy: 0.078
Train accuracy: 0.085
Epoch 2: loss=129453.05425
Test accuracy: 0.090
Train accuracy: 0.084
Epoch 3: loss=129173.81771
Test accuracy: 0.086
Train accuracy: 0.083
Epoch 4: loss=128895.12066
Test accuracy: 0.078
Train accuracy: 0.084
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=446339.82465
Test accuracy: 0.082
Train accuracy: 0.083
Epoch 2: loss=445377.23438
Test accuracy: 0.086
Train accuracy: 0.083
Epoch 3: loss=444416.31771
Test accuracy: 0.078
Train accuracy: 0.082
Epoch 4: loss=443457.21701
Test accuracy: 0.082
Train accuracy: 0.081
{'quantization.global_wb': 3, 'inp_mult': 200, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=1535840.41667
Test accuracy: 0.090
Train accuracy: 0.083
Epoch 2: loss=1532527.07639
Test accuracy: 0.086
Train accuracy: 0.082
Epoch 3: loss=1529219.41667
Test accuracy: 0.078
Train accuracy: 0.082
Epoch 4: loss=1525917.98611
Test accuracy: 0.082
Train accuracy: 0.083

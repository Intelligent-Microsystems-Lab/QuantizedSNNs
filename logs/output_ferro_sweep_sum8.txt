Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Traceback (most recent call last):
  File "sweep_sumDVS.py", line 128, in <module>
    test_data = pickle.load(f)
MemoryError
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
hello
hello
hello
hello
hello
hello1
hello1
hello1
hello1
hello1
Traceback (most recent call last):
  File "sweep_sumDVS.py", line 137, in <module>
    x_train = x_train.drop_duplicates()
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/pandas/core/frame.py", line 4630, in drop_duplicates
    duplicated = self.duplicated(subset, keep=keep)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/pandas/core/frame.py", line 4687, in duplicated
    labels, shape = map(list, zip(*map(f, vals)))
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/pandas/core/frame.py", line 4668, in f
    vals, size_hint=min(len(self), _SIZE_HINT_LIMIT))
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/pandas/util/_decorators.py", line 188, in wrapper
    return func(*args, **kwargs)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/pandas/core/algorithms.py", line 613, in factorize
    na_value=na_value)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/pandas/core/algorithms.py", line 460, in _factorize_array
    na_value=na_value)
  File "pandas/_libs/hashtable_class_helper.pxi", line 1209, in pandas._libs.hashtable.Int64HashTable.factorize
  File "pandas/_libs/hashtable_class_helper.pxi", line 1104, in pandas._libs.hashtable.Int64HashTable._unique
numpy.core._exceptions.MemoryError: Unable to allocate array with shape (104249103,) and data type int64
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
init done
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00011200000000000001, 9.82e-05]}
Epoch 1: loss=363635733.80936
Test accuracy: 0.223
Train accuracy: 0.212
Epoch 2: loss=12.60594
Test accuracy: 0.168
Train accuracy: 0.287
Epoch 3: loss=9341.67198
Test accuracy: 0.098
Train accuracy: 0.199
Epoch 4: loss=2556.14230
Test accuracy: 0.098
Train accuracy: 0.198
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00027556290188270857, 0.00024160961575787483]}
Epoch 1: loss=148.10071
Test accuracy: 0.125
Train accuracy: 0.161
Epoch 2: loss=10.66550
Test accuracy: 0.141
Train accuracy: 0.184
Epoch 3: loss=9.02608
Test accuracy: 0.148
Train accuracy: 0.245
Epoch 4: loss=8.21543
Test accuracy: 0.145
Train accuracy: 0.312
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0006779902936966005, 0.0005944522039375551]}
Epoch 1: loss=14.90644
Test accuracy: 0.223
Train accuracy: 0.265
Epoch 2: loss=97083050053.17982
Test accuracy: 0.324
Train accuracy: 0.371
Epoch 3: loss=1398214.90211
Test accuracy: 0.289
Train accuracy: 0.399
Epoch 4: loss=2047479.31582
Test accuracy: 0.285
Train accuracy: 0.397
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.001668115828387008, 0.0014625801281036087]}
Epoch 1: loss=16.17924
Test accuracy: 0.176
Train accuracy: 0.194
Epoch 2: loss=8.41916
Test accuracy: 0.164
Train accuracy: 0.258
Epoch 3: loss=162177.60751
Test accuracy: 0.238
Train accuracy: 0.368
Epoch 4: loss=409.71013
Test accuracy: 0.152
Train accuracy: 0.342
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0041042039138107335, 0.003598507360144768]}
Epoch 1: loss=12.59441
Test accuracy: 0.176
Train accuracy: 0.205
Epoch 2: loss=9907153588.76394
Test accuracy: 0.258
Train accuracy: 0.354
Epoch 3: loss=60843283863813.71094
Test accuracy: 0.250
Train accuracy: 0.461
Epoch 4: loss=501601161.24557
Test accuracy: 0.102
Train accuracy: 0.344
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.010097913753643354, 0.008853706523283726]}
Epoch 1: loss=9.79936
Test accuracy: 0.215
Train accuracy: 0.260
Epoch 2: loss=6.43443
Test accuracy: 0.258
Train accuracy: 0.424
Epoch 3: loss=5.73886
Test accuracy: 0.262
Train accuracy: 0.609
Epoch 4: loss=5.34006
Test accuracy: 0.277
Train accuracy: 0.740
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.024844735865314944, 0.021783509481910067]}
Traceback (most recent call last):
  File "sweep_sumDVS.py", line 407, in <module>
    loss_hist, test_acc, train_acc, best = train(x_train, y_train, lr = quantization.global_lr, nb_epochs = 4)
  File "sweep_sumDVS.py", line 348, in train
    loss_val.backward()
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 11.75 GiB total capacity; 9.33 GiB already allocated; 73.75 MiB free; 1.21 GiB cached)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00011200000000000001, 9.82e-05]}
Epoch 1: loss=2036.92451
Test accuracy: 0.074
Train accuracy: 0.103
Epoch 2: loss=9355573405603885056.00000
Test accuracy: 0.113
Train accuracy: 0.116
Epoch 3: loss=9332.63042
Test accuracy: 0.125
Train accuracy: 0.174
Epoch 4: loss=12.64075
Test accuracy: 0.141
Train accuracy: 0.218
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00027556290188270857, 0.00024160961575787483]}
Epoch 1: loss=109312.63389
Test accuracy: 0.102
Train accuracy: 0.070
Epoch 2: loss=26.98091
Test accuracy: 0.109
Train accuracy: 0.077
Epoch 3: loss=23.63101
Test accuracy: 0.113
Train accuracy: 0.074
Epoch 4: loss=45927055414369.92188
Test accuracy: 0.102
Train accuracy: 0.072
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0006779902936966005, 0.0005944522039375551]}
Epoch 1: loss=19.32932
Test accuracy: 0.207
Train accuracy: 0.184
Epoch 2: loss=9.19094
Test accuracy: 0.191
Train accuracy: 0.239
Epoch 3: loss=7.69792
Test accuracy: 0.227
Train accuracy: 0.319
Epoch 4: loss=6.97488
Test accuracy: 0.238
Train accuracy: 0.386
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.001668115828387008, 0.0014625801281036087]}
Epoch 1: loss=2840.33563
Test accuracy: 0.070
Train accuracy: 0.089
Epoch 2: loss=9.60936
Test accuracy: 0.195
Train accuracy: 0.181
Epoch 3: loss=7.48096
Test accuracy: 0.273
Train accuracy: 0.332
Epoch 4: loss=6.87741
Test accuracy: 0.273
Train accuracy: 0.399
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0041042039138107335, 0.003598507360144768]}
Epoch 1: loss=51603.61726
Test accuracy: 0.094
Train accuracy: 0.089
Epoch 2: loss=13.84134
Test accuracy: 0.160
Train accuracy: 0.131
Epoch 3: loss=136877.64442
Test accuracy: 0.137
Train accuracy: 0.162
Epoch 4: loss=14.66881
Test accuracy: 0.090
Train accuracy: 0.102
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.010097913753643354, 0.008853706523283726]}
Epoch 1: loss=12.16962
Test accuracy: 0.270
Train accuracy: 0.241
Epoch 2: loss=7.20274
Test accuracy: 0.293
Train accuracy: 0.349
Epoch 3: loss=6.36836
Test accuracy: 0.277
Train accuracy: 0.465
Epoch 4: loss=5.88343
Test accuracy: 0.289
Train accuracy: 0.568
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.024844735865314944, 0.021783509481910067]}
Epoch 1: loss=13.64735
Test accuracy: 0.180
Train accuracy: 0.194
Epoch 2: loss=7.67263
Test accuracy: 0.250
Train accuracy: 0.304
Epoch 3: loss=6.59807
Test accuracy: 0.277
Train accuracy: 0.426
Epoch 4: loss=6.04198
Test accuracy: 0.297
Train accuracy: 0.506
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.06112756706746064, 0.05359577755379138]}
Epoch 1: loss=21.44754
Test accuracy: 0.273
Train accuracy: 0.224
Epoch 2: loss=8.67953
Test accuracy: 0.289
Train accuracy: 0.322
Epoch 3: loss=7.07752
Test accuracy: 0.277
Train accuracy: 0.401
Epoch 4: loss=6.36561
Test accuracy: 0.301
Train accuracy: 0.468
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.15039723005481556, 0.13186614278020434]}
Epoch 1: loss=53.06623
Test accuracy: 0.121
Train accuracy: 0.099
Epoch 2: loss=2250.04419
Test accuracy: 0.238
Train accuracy: 0.154
Epoch 3: loss=8.72956
Test accuracy: 0.285
Train accuracy: 0.326
Epoch 4: loss=7.90505
Test accuracy: 0.266
Train accuracy: 0.345
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.37003479597344896, 0.324441222898149]}
Epoch 1: loss=15.12826
Test accuracy: 0.289
Train accuracy: 0.236
Epoch 2: loss=6.83304
Test accuracy: 0.293
Train accuracy: 0.368
Epoch 3: loss=1295.59819
Test accuracy: 0.312
Train accuracy: 0.434
Epoch 4: loss=8870.27480
Test accuracy: 0.309
Train accuracy: 0.446
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.9104273408573178, 0.7982496863588268]}
Epoch 1: loss=11.20155
Test accuracy: 0.219
Train accuracy: 0.212
Epoch 2: loss=52.75267
Test accuracy: 0.188
Train accuracy: 0.312
Epoch 3: loss=1437.25465
Test accuracy: 0.246
Train accuracy: 0.297
Epoch 4: loss=933.78824
Test accuracy: 0.293
Train accuracy: 0.375
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.24, 1.964]}
Epoch 1: loss=15.84530
Test accuracy: 0.109
Train accuracy: 0.130
Epoch 2: loss=1401869093393826.00000
Test accuracy: 0.293
Train accuracy: 0.212
Epoch 3: loss=2009933834314.33105
Test accuracy: 0.340
Train accuracy: 0.382
Epoch 4: loss=412479953989700632117772288.00000
Test accuracy: 0.324
Train accuracy: 0.377

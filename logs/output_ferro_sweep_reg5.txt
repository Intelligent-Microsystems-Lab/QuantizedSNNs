Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
python: can't open file 'sweep_regDVS.py': [Errno 2] No such file or directory
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
init done
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=3.56674
Test accuracy: 0.234
Train accuracy: 0.161
Epoch 2: loss=2.63223
Test accuracy: 0.258
Train accuracy: 0.242
Epoch 3: loss=2.48797
Test accuracy: 0.301
Train accuracy: 0.272
Epoch 4: loss=2.21288
Test accuracy: 0.395
Train accuracy: 0.335
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=4.45761
Test accuracy: 0.270
Train accuracy: 0.104
Epoch 2: loss=3.43126
Test accuracy: 0.281
Train accuracy: 0.294
Epoch 3: loss=3.23061
Test accuracy: 0.359
Train accuracy: 0.315
Epoch 4: loss=2.78229
Test accuracy: 0.348
Train accuracy: 0.411
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=7.07152
Test accuracy: 0.129
Train accuracy: 0.092
Epoch 2: loss=5.48919
Test accuracy: 0.156
Train accuracy: 0.141
Epoch 3: loss=5.16680
Test accuracy: 0.168
Train accuracy: 0.181
Epoch 4: loss=4.59558
Test accuracy: 0.211
Train accuracy: 0.284
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=11.07345
Test accuracy: 0.195
Train accuracy: 0.097
Epoch 2: loss=10.18357
Test accuracy: 0.203
Train accuracy: 0.250
Epoch 3: loss=10.03624
Test accuracy: 0.230
Train accuracy: 0.268
Epoch 4: loss=9.73481
Test accuracy: 0.305
Train accuracy: 0.337
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=29.37841
Test accuracy: 0.156
Train accuracy: 0.089
Epoch 2: loss=28.59984
Test accuracy: 0.188
Train accuracy: 0.268
Epoch 3: loss=28.48217
Test accuracy: 0.238
Train accuracy: 0.305
Epoch 4: loss=28.28957
Test accuracy: 0.285
Train accuracy: 0.363
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Traceback (most recent call last):
  File "swep_regDVS.py", line 403, in <module>
    loss_hist, test_acc, train_acc, best = train(x_train, y_train, lr = quantization.global_lr, nb_epochs = 4)
  File "swep_regDVS.py", line 348, in train
    loss_val.backward()
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 10.73 GiB total capacity; 8.60 GiB already allocated; 129.56 MiB free; 1.09 GiB cached)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=3.75049
Test accuracy: 0.234
Train accuracy: 0.109
Epoch 2: loss=2.77006
Test accuracy: 0.211
Train accuracy: 0.240
Epoch 3: loss=2.58534
Test accuracy: 0.234
Train accuracy: 0.255
Epoch 4: loss=2.26396
Test accuracy: 0.305
Train accuracy: 0.315
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=3.77029
Test accuracy: 0.211
Train accuracy: 0.110
Epoch 2: loss=3.01451
Test accuracy: 0.262
Train accuracy: 0.257
Epoch 3: loss=2.88052
Test accuracy: 0.270
Train accuracy: 0.293
Epoch 4: loss=2.65869
Test accuracy: 0.340
Train accuracy: 0.349
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=5.60530
Test accuracy: 0.180
Train accuracy: 0.131
Epoch 2: loss=4.53592
Test accuracy: 0.242
Train accuracy: 0.281
Epoch 3: loss=4.38598
Test accuracy: 0.301
Train accuracy: 0.313
Epoch 4: loss=4.06249
Test accuracy: 0.367
Train accuracy: 0.435
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=11.19876
Test accuracy: 0.203
Train accuracy: 0.080
Epoch 2: loss=10.14127
Test accuracy: 0.250
Train accuracy: 0.270
Epoch 3: loss=9.98868
Test accuracy: 0.297
Train accuracy: 0.293
Epoch 4: loss=9.65406
Test accuracy: 0.355
Train accuracy: 0.373
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=29.84511
Test accuracy: 0.148
Train accuracy: 0.107
Epoch 2: loss=28.78813
Test accuracy: 0.215
Train accuracy: 0.205
Epoch 3: loss=28.64942
Test accuracy: 0.301
Train accuracy: 0.244
Epoch 4: loss=28.34343
Test accuracy: 0.383
Train accuracy: 0.359
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=94.65432
Test accuracy: 0.129
Train accuracy: 0.117
Epoch 2: loss=93.34331
Test accuracy: 0.148
Train accuracy: 0.182
Epoch 3: loss=93.10886
Test accuracy: 0.207
Train accuracy: 0.220
Epoch 4: loss=92.65953
Test accuracy: 0.277
Train accuracy: 0.304
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=315.12733
Test accuracy: 0.289
Train accuracy: 0.083
Epoch 2: loss=313.77506
Test accuracy: 0.324
Train accuracy: 0.285
Epoch 3: loss=313.20532
Test accuracy: 0.324
Train accuracy: 0.316
Epoch 4: loss=312.50588
Test accuracy: 0.363
Train accuracy: 0.391
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=1076.39233
Test accuracy: 0.129
Train accuracy: 0.088
Epoch 2: loss=1072.12528
Test accuracy: 0.180
Train accuracy: 0.235
Epoch 3: loss=1068.43403
Test accuracy: 0.219
Train accuracy: 0.255
Epoch 4: loss=1064.47841
Test accuracy: 0.305
Train accuracy: 0.312
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=3690.16843
Test accuracy: 0.180
Train accuracy: 0.085
Epoch 2: loss=3668.73465
Test accuracy: 0.258
Train accuracy: 0.264
Epoch 3: loss=3647.11707
Test accuracy: 0.312
Train accuracy: 0.288
Epoch 4: loss=3624.93085
Test accuracy: 0.336
Train accuracy: 0.386
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=12675.38824
Test accuracy: 0.199
Train accuracy: 0.095
Epoch 2: loss=12578.46181
Test accuracy: 0.230
Train accuracy: 0.200
Epoch 3: loss=12480.33366
Test accuracy: 0.277
Train accuracy: 0.234
Epoch 4: loss=12381.07541
Test accuracy: 0.355
Train accuracy: 0.365
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=43585.91580
Test accuracy: 0.195
Train accuracy: 0.075
Epoch 2: loss=43208.65538
Test accuracy: 0.219
Train accuracy: 0.258
Epoch 3: loss=42832.70855
Test accuracy: 0.262
Train accuracy: 0.287
Epoch 4: loss=42459.13433
Test accuracy: 0.305
Train accuracy: 0.338
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=149924.48438
Test accuracy: 0.320
Train accuracy: 0.103
Epoch 2: loss=148607.20660
Test accuracy: 0.367
Train accuracy: 0.334
Epoch 3: loss=147299.62413
Test accuracy: 0.352
Train accuracy: 0.355
Epoch 4: loss=146002.20312
Test accuracy: 0.352
Train accuracy: 0.412

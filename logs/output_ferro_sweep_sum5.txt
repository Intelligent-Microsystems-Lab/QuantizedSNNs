Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Traceback (most recent call last):
  File "sweep_sumDVS.py", line 126, in <module>
    train_data = pickle.load(f)
MemoryError
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Traceback (most recent call last):
  File "sweep_sumDVS.py", line 126, in <module>
    train_data = pickle.load(f)
MemoryError
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
init done
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00011200000000000001, 9.82e-05]}
Epoch 1: loss=196.21486
Test accuracy: 0.090
Train accuracy: 0.089
Epoch 2: loss=195.01000
Test accuracy: 0.117
Train accuracy: 0.155
Epoch 3: loss=194.62295
Test accuracy: 0.176
Train accuracy: 0.199
Epoch 4: loss=194.12159
Test accuracy: 0.270
Train accuracy: 0.298
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00027556290188270857, 0.00024160961575787483]}
Epoch 1: loss=196.49051
Test accuracy: 0.180
Train accuracy: 0.101
Epoch 2: loss=195.09451
Test accuracy: 0.207
Train accuracy: 0.206
Epoch 3: loss=194.77361
Test accuracy: 0.250
Train accuracy: 0.243
Epoch 4: loss=194.23906
Test accuracy: 0.301
Train accuracy: 0.336
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0006779902936966005, 0.0005944522039375551]}
Epoch 1: loss=195.55992
Test accuracy: 0.230
Train accuracy: 0.169
Epoch 2: loss=194.66741
Test accuracy: 0.250
Train accuracy: 0.285
Epoch 3: loss=194.35205
Test accuracy: 0.281
Train accuracy: 0.309
Epoch 4: loss=193.97793
Test accuracy: 0.344
Train accuracy: 0.363
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.001668115828387008, 0.0014625801281036087]}
Epoch 1: loss=196.37681
Test accuracy: 0.195
Train accuracy: 0.101
Epoch 2: loss=195.03881
Test accuracy: 0.184
Train accuracy: 0.229
Epoch 3: loss=194.69284
Test accuracy: 0.246
Train accuracy: 0.247
Epoch 4: loss=194.15007
Test accuracy: 0.320
Train accuracy: 0.345
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0041042039138107335, 0.003598507360144768]}
Epoch 1: loss=195.57848
Test accuracy: 0.223
Train accuracy: 0.124
Epoch 2: loss=194.55502
Test accuracy: 0.285
Train accuracy: 0.314
Epoch 3: loss=194.23679
Test accuracy: 0.340
Train accuracy: 0.344
Epoch 4: loss=193.79039
Test accuracy: 0.367
Train accuracy: 0.416
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.010097913753643354, 0.008853706523283726]}
Traceback (most recent call last):
  File "sweep_sumDVS.py", line 407, in <module>
    loss_hist, test_acc, train_acc, best = train(x_train, y_train, lr = quantization.global_lr, nb_epochs = 4)
  File "sweep_sumDVS.py", line 348, in train
    loss_val.backward()
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 10.73 GiB total capacity; 8.60 GiB already allocated; 177.56 MiB free; 1.04 GiB cached)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00011200000000000001, 9.82e-05]}
Epoch 1: loss=195.72528
Test accuracy: 0.188
Train accuracy: 0.084
Epoch 2: loss=194.62985
Test accuracy: 0.254
Train accuracy: 0.280
Epoch 3: loss=194.33944
Test accuracy: 0.277
Train accuracy: 0.309
Epoch 4: loss=193.93533
Test accuracy: 0.355
Train accuracy: 0.350
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00027556290188270857, 0.00024160961575787483]}
Epoch 1: loss=196.21982
Test accuracy: 0.141
Train accuracy: 0.065
Epoch 2: loss=194.97539
Test accuracy: 0.180
Train accuracy: 0.185
Epoch 3: loss=194.62503
Test accuracy: 0.266
Train accuracy: 0.221
Epoch 4: loss=194.16325
Test accuracy: 0.340
Train accuracy: 0.329
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0006779902936966005, 0.0005944522039375551]}
Epoch 1: loss=196.13515
Test accuracy: 0.180
Train accuracy: 0.105
Epoch 2: loss=194.81219
Test accuracy: 0.211
Train accuracy: 0.258
Epoch 3: loss=194.50730
Test accuracy: 0.262
Train accuracy: 0.286
Epoch 4: loss=194.02444
Test accuracy: 0.328
Train accuracy: 0.388
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.001668115828387008, 0.0014625801281036087]}
Epoch 1: loss=195.78653
Test accuracy: 0.148
Train accuracy: 0.107
Epoch 2: loss=194.71654
Test accuracy: 0.176
Train accuracy: 0.232
Epoch 3: loss=194.41657
Test accuracy: 0.230
Train accuracy: 0.274
Epoch 4: loss=193.95488
Test accuracy: 0.270
Train accuracy: 0.353
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0041042039138107335, 0.003598507360144768]}
Epoch 1: loss=196.27785
Test accuracy: 0.137
Train accuracy: 0.082
Epoch 2: loss=195.05975
Test accuracy: 0.137
Train accuracy: 0.145
Epoch 3: loss=194.70648
Test accuracy: 0.277
Train accuracy: 0.190
Epoch 4: loss=194.14087
Test accuracy: 0.309
Train accuracy: 0.289
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.010097913753643354, 0.008853706523283726]}
Epoch 1: loss=197.60148
Test accuracy: 0.125
Train accuracy: 0.082
Epoch 2: loss=195.76395
Test accuracy: 0.141
Train accuracy: 0.136
Epoch 3: loss=195.27920
Test accuracy: 0.188
Train accuracy: 0.138
Epoch 4: loss=194.39936
Test accuracy: 0.246
Train accuracy: 0.246
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.024844735865314944, 0.021783509481910067]}
Epoch 1: loss=196.02159
Test accuracy: 0.098
Train accuracy: 0.163
Epoch 2: loss=194.88479
Test accuracy: 0.164
Train accuracy: 0.161
Epoch 3: loss=194.52465
Test accuracy: 0.227
Train accuracy: 0.218
Epoch 4: loss=194.08610
Test accuracy: 0.289
Train accuracy: 0.286
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.06112756706746064, 0.05359577755379138]}
Epoch 1: loss=196.36763
Test accuracy: 0.070
Train accuracy: 0.090
Epoch 2: loss=194.96699
Test accuracy: 0.086
Train accuracy: 0.102
Epoch 3: loss=194.58363
Test accuracy: 0.176
Train accuracy: 0.142
Epoch 4: loss=194.00168
Test accuracy: 0.223
Train accuracy: 0.266
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.15039723005481556, 0.13186614278020434]}
Epoch 1: loss=195.44663
Test accuracy: 0.195
Train accuracy: 0.097
Epoch 2: loss=194.18941
Test accuracy: 0.215
Train accuracy: 0.218
Epoch 3: loss=193.89476
Test accuracy: 0.254
Train accuracy: 0.234
Epoch 4: loss=193.44760
Test accuracy: 0.383
Train accuracy: 0.323
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.37003479597344896, 0.324441222898149]}
Epoch 1: loss=195.25371
Test accuracy: 0.180
Train accuracy: 0.104
Epoch 2: loss=193.98637
Test accuracy: 0.188
Train accuracy: 0.218
Epoch 3: loss=193.61627
Test accuracy: 0.234
Train accuracy: 0.231
Epoch 4: loss=193.09806
Test accuracy: 0.293
Train accuracy: 0.293
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.9104273408573178, 0.7982496863588268]}
Epoch 1: loss=192.52312
Test accuracy: 0.207
Train accuracy: 0.103
Epoch 2: loss=191.31153
Test accuracy: 0.238
Train accuracy: 0.245
Epoch 3: loss=190.96049
Test accuracy: 0.254
Train accuracy: 0.264
Epoch 4: loss=190.43773
Test accuracy: 0.344
Train accuracy: 0.354
{'quantization.global_wb': 5, 'inp_mult': 150, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.24, 1.964]}
Epoch 1: loss=187.04700
Test accuracy: 0.211
Train accuracy: 0.102
Epoch 2: loss=185.93307
Test accuracy: 0.211
Train accuracy: 0.252
Epoch 3: loss=185.59415
Test accuracy: 0.262
Train accuracy: 0.284
Epoch 4: loss=185.08151
Test accuracy: 0.262
Train accuracy: 0.349

Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Killed
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
init done
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00011200000000000001, 9.82e-05]}
Epoch 1: loss=771.27271
Test accuracy: 0.086
Train accuracy: 0.095
Epoch 2: loss=770.34655
Test accuracy: 0.188
Train accuracy: 0.155
Epoch 3: loss=769.40641
Test accuracy: 0.191
Train accuracy: 0.214
Epoch 4: loss=768.52793
Test accuracy: 0.195
Train accuracy: 0.224
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00027556290188270857, 0.00024160961575787483]}
Epoch 1: loss=771.11516
Test accuracy: 0.105
Train accuracy: 0.094
Epoch 2: loss=770.18414
Test accuracy: 0.168
Train accuracy: 0.145
Epoch 3: loss=769.23753
Test accuracy: 0.172
Train accuracy: 0.186
Epoch 4: loss=768.35010
Test accuracy: 0.184
Train accuracy: 0.189
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0006779902936966005, 0.0005944522039375551]}
Epoch 1: loss=771.33343
Test accuracy: 0.090
Train accuracy: 0.083
Epoch 2: loss=770.39891
Test accuracy: 0.117
Train accuracy: 0.088
Epoch 3: loss=769.45308
Test accuracy: 0.090
Train accuracy: 0.097
Epoch 4: loss=768.56426
Test accuracy: 0.109
Train accuracy: 0.102
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.001668115828387008, 0.0014625801281036087]}
Epoch 1: loss=771.60974
Test accuracy: 0.090
Train accuracy: 0.083
Epoch 2: loss=770.71461
Test accuracy: 0.098
Train accuracy: 0.082
Epoch 3: loss=769.80264
Test accuracy: 0.098
Train accuracy: 0.089
Epoch 4: loss=768.97632
Test accuracy: 0.078
Train accuracy: 0.093
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0041042039138107335, 0.003598507360144768]}
Epoch 1: loss=771.41985
Test accuracy: 0.078
Train accuracy: 0.083
Epoch 2: loss=770.52782
Test accuracy: 0.078
Train accuracy: 0.082
Epoch 3: loss=769.65057
Test accuracy: 0.090
Train accuracy: 0.082
Epoch 4: loss=768.83905
Test accuracy: 0.090
Train accuracy: 0.086
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.010097913753643354, 0.008853706523283726]}
Epoch 1: loss=771.43042
Test accuracy: 0.070
Train accuracy: 0.083
Epoch 2: loss=770.55156
Test accuracy: 0.086
Train accuracy: 0.082
Epoch 3: loss=769.64788
Test accuracy: 0.082
Train accuracy: 0.084
Epoch 4: loss=768.82058
Test accuracy: 0.078
Train accuracy: 0.084
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.024844735865314944, 0.021783509481910067]}
Traceback (most recent call last):
  File "sweep_sumDVS.py", line 407, in <module>
    loss_hist, test_acc, train_acc, best = train(x_train, y_train, lr = quantization.global_lr, nb_epochs = 4)
  File "sweep_sumDVS.py", line 348, in train
    loss_val.backward()
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 11.75 GiB total capacity; 9.34 GiB already allocated; 73.75 MiB free; 1.21 GiB cached)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00011200000000000001, 9.82e-05]}
Epoch 1: loss=771.17812
Test accuracy: 0.078
Train accuracy: 0.076
Epoch 2: loss=770.28121
Test accuracy: 0.152
Train accuracy: 0.119
Epoch 3: loss=769.35848
Test accuracy: 0.156
Train accuracy: 0.186
Epoch 4: loss=768.50633
Test accuracy: 0.160
Train accuracy: 0.193
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.00027556290188270857, 0.00024160961575787483]}
Epoch 1: loss=771.57099
Test accuracy: 0.078
Train accuracy: 0.084
Epoch 2: loss=770.67836
Test accuracy: 0.082
Train accuracy: 0.083
Epoch 3: loss=769.78562
Test accuracy: 0.082
Train accuracy: 0.085
Epoch 4: loss=768.97362
Test accuracy: 0.086
Train accuracy: 0.084
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0006779902936966005, 0.0005944522039375551]}
Epoch 1: loss=771.22722
Test accuracy: 0.055
Train accuracy: 0.042
Epoch 2: loss=770.31042
Test accuracy: 0.074
Train accuracy: 0.059
Epoch 3: loss=769.38783
Test accuracy: 0.105
Train accuracy: 0.097
Epoch 4: loss=768.52048
Test accuracy: 0.105
Train accuracy: 0.121
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.001668115828387008, 0.0014625801281036087]}
Epoch 1: loss=771.62188
Test accuracy: 0.090
Train accuracy: 0.082
Epoch 2: loss=770.71030
Test accuracy: 0.129
Train accuracy: 0.095
Epoch 3: loss=769.80991
Test accuracy: 0.137
Train accuracy: 0.108
Epoch 4: loss=768.97896
Test accuracy: 0.129
Train accuracy: 0.112
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.0041042039138107335, 0.003598507360144768]}
Epoch 1: loss=771.31676
Test accuracy: 0.094
Train accuracy: 0.086
Epoch 2: loss=770.39009
Test accuracy: 0.086
Train accuracy: 0.092
Epoch 3: loss=769.45801
Test accuracy: 0.098
Train accuracy: 0.107
Epoch 4: loss=768.59775
Test accuracy: 0.094
Train accuracy: 0.109
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.010097913753643354, 0.008853706523283726]}
Epoch 1: loss=771.65198
Test accuracy: 0.078
Train accuracy: 0.083
Epoch 2: loss=770.75142
Test accuracy: 0.090
Train accuracy: 0.083
Epoch 3: loss=769.84667
Test accuracy: 0.090
Train accuracy: 0.087
Epoch 4: loss=769.03417
Test accuracy: 0.086
Train accuracy: 0.088
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.024844735865314944, 0.021783509481910067]}
Epoch 1: loss=771.45721
Test accuracy: 0.094
Train accuracy: 0.084
Epoch 2: loss=770.56855
Test accuracy: 0.090
Train accuracy: 0.084
Epoch 3: loss=769.67156
Test accuracy: 0.094
Train accuracy: 0.092
Epoch 4: loss=768.86440
Test accuracy: 0.090
Train accuracy: 0.091
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.06112756706746064, 0.05359577755379138]}
Epoch 1: loss=771.22103
Test accuracy: 0.090
Train accuracy: 0.084
Epoch 2: loss=770.34612
Test accuracy: 0.082
Train accuracy: 0.087
Epoch 3: loss=769.46601
Test accuracy: 0.082
Train accuracy: 0.091
Epoch 4: loss=768.66547
Test accuracy: 0.082
Train accuracy: 0.093
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.15039723005481556, 0.13186614278020434]}
Epoch 1: loss=770.82472
Test accuracy: 0.098
Train accuracy: 0.105
Epoch 2: loss=769.90005
Test accuracy: 0.125
Train accuracy: 0.106
Epoch 3: loss=768.97136
Test accuracy: 0.117
Train accuracy: 0.118
Epoch 4: loss=768.11534
Test accuracy: 0.129
Train accuracy: 0.123
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.37003479597344896, 0.324441222898149]}
Epoch 1: loss=769.73165
Test accuracy: 0.086
Train accuracy: 0.084
Epoch 2: loss=768.83278
Test accuracy: 0.117
Train accuracy: 0.093
Epoch 3: loss=767.92093
Test accuracy: 0.137
Train accuracy: 0.111
Epoch 4: loss=767.07652
Test accuracy: 0.121
Train accuracy: 0.122
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [0.9104273408573178, 0.7982496863588268]}
Epoch 1: loss=767.61475
Test accuracy: 0.078
Train accuracy: 0.087
Epoch 2: loss=766.67601
Test accuracy: 0.141
Train accuracy: 0.138
Epoch 3: loss=765.72257
Test accuracy: 0.184
Train accuracy: 0.227
Epoch 4: loss=764.83821
Test accuracy: 0.160
Train accuracy: 0.230
{'quantization.global_wb': 4, 'inp_mult': 180, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.24, 1.964]}
Epoch 1: loss=762.27105
Test accuracy: 0.090
Train accuracy: 0.085
Epoch 2: loss=761.38419
Test accuracy: 0.086
Train accuracy: 0.083
Epoch 3: loss=760.46610
Test accuracy: 0.105
Train accuracy: 0.087
Epoch 4: loss=759.59450
Test accuracy: 0.090
Train accuracy: 0.092

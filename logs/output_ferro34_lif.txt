Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
init done
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1e-05, 'reg_size': 0.00048219502065459076, 'mult_eq': 0.12, 'class_method': 'integrate'}
Traceback (most recent call last):
  File "spytorch_dvs2_LIF.py", line 476, in <module>
    loss_hist, test_acc, train_acc, best = train(x_train, y_train, lr = quantization.global_lr, nb_epochs = 35)
  File "spytorch_dvs2_LIF.py", line 425, in train
    loss_val.backward()
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 158.00 MiB (GPU 0; 10.73 GiB total capacity; 2.52 GiB already allocated; 26.56 MiB free; 476.14 MiB cached)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
init done
{'quantization.global_wb': 34, 'inp_mult': 80, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1e-05, 'reg_size': 0.00048219502065459076, 'mult_eq': 0.12, 'class_method': 'integrate'}
Epoch 1: loss=3793.72626
Test accuracy: 0.258
Train accuracy: 0.230
Epoch 2: loss=1205.89267
Test accuracy: 0.285
Train accuracy: 0.458
Epoch 3: loss=559.89647
Test accuracy: 0.309
Train accuracy: 0.666
Epoch 4: loss=272.11597
Test accuracy: 0.316
Train accuracy: 0.782
Epoch 5: loss=118.40199
Test accuracy: 0.277
Train accuracy: 0.869
Epoch 6: loss=66.15027
Test accuracy: 0.312
Train accuracy: 0.919
Epoch 7: loss=33.35351
Test accuracy: 0.332
Train accuracy: 0.944
Epoch 8: loss=21.39625
Test accuracy: 0.316
Train accuracy: 0.957
Epoch 9: loss=12.82326
Test accuracy: 0.305
Train accuracy: 0.964
Epoch 10: loss=7.65190
Test accuracy: 0.316
Train accuracy: 0.984
Epoch 11: loss=4.85532
Test accuracy: 0.316
Train accuracy: 0.994
Epoch 12: loss=4.55134
Test accuracy: 0.324
Train accuracy: 0.997
Epoch 13: loss=4.21512
Test accuracy: 0.312
Train accuracy: 0.997
Epoch 14: loss=4.03598
Test accuracy: 0.320
Train accuracy: 0.998
Epoch 15: loss=3.83125
Test accuracy: 0.305
Train accuracy: 0.998
Epoch 16: loss=3.72743
Test accuracy: 0.301
Train accuracy: 0.998
Epoch 17: loss=3.58229
Test accuracy: 0.336
Train accuracy: 0.998
Epoch 18: loss=3.51589
Test accuracy: 0.340
Train accuracy: 0.998
Epoch 19: loss=3.40838
Test accuracy: 0.324
Train accuracy: 0.998
Epoch 20: loss=3.38042
Test accuracy: 0.336
Train accuracy: 0.998
Epoch 21: loss=3.35572
Test accuracy: 0.309
Train accuracy: 0.998
Epoch 22: loss=3.33906
Test accuracy: 0.324
Train accuracy: 0.998
Epoch 23: loss=3.32094
Test accuracy: 0.332
Train accuracy: 0.998
Epoch 24: loss=3.31511
Test accuracy: 0.301
Train accuracy: 0.998
Epoch 25: loss=3.28911
Test accuracy: 0.305
Train accuracy: 0.998
Epoch 26: loss=3.24353
Test accuracy: 0.320
Train accuracy: 0.998
Epoch 27: loss=3.22637
Test accuracy: 0.328
Train accuracy: 0.998
Epoch 28: loss=3.20992
Test accuracy: 0.320
Train accuracy: 0.998
Epoch 29: loss=3.20957
Test accuracy: 0.328
Train accuracy: 0.998
Epoch 30: loss=3.20921
Test accuracy: 0.320
Train accuracy: 0.998
Epoch 31: loss=3.19809
Test accuracy: 0.324
Train accuracy: 0.998
Epoch 32: loss=3.19561
Test accuracy: 0.320
Train accuracy: 0.998
Epoch 33: loss=3.19520
Test accuracy: 0.328
Train accuracy: 0.998
Epoch 34: loss=3.19481
Test accuracy: 0.309
Train accuracy: 0.998
Epoch 35: loss=3.18987
Test accuracy: 0.301
Train accuracy: 0.998
{'quantization.global_wb': 34, 'inp_mult': 80, 'reg_size': 0.00048219502065459076, 'weight_sum': 1.12}

Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
python: can't open file 'sweep_regDVS.py': [Errno 2] No such file or directory
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Traceback (most recent call last):
  File "swep_regDVS.py", line 126, in <module>
    train_data = pickle.load(f)
MemoryError
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
hello
hello
hello
hello
hello
hello
hello1
hello1
hello1
hello1
hello1
init done
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=29320.57969
Test accuracy: 0.113
Train accuracy: 0.152
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=978485354030876348907520.00000
Test accuracy: 0.137
Train accuracy: 0.116
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 1500, 'nb_steps': 150, 'batch_size': 128, 'quantization.global_lr': 0.0004, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [2.1, 0.003]}
Epoch 1: loss=213047.24624
Test accuracy: 0.176
Train accuracy: 0.218
Epoch 2: loss=148181.10085
Test accuracy: 0.172
Train accuracy: 0.249
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
init done
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=6.37009
Test accuracy: 0.258
Train accuracy: 0.234
Epoch 2: loss=2.15244
Test accuracy: 0.289
Train accuracy: 0.411
Epoch 3: loss=1.45360
Test accuracy: 0.305
Train accuracy: 0.575
Epoch 4: loss=1.02663
Test accuracy: 0.328
Train accuracy: 0.722
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=6.10427
Test accuracy: 0.273
Train accuracy: 0.285
Epoch 2: loss=2.23161
Test accuracy: 0.270
Train accuracy: 0.459
Epoch 3: loss=1.46115
Test accuracy: 0.273
Train accuracy: 0.606
Epoch 4: loss=1.07302
Test accuracy: 0.328
Train accuracy: 0.722
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=25.35279
Test accuracy: 0.164
Train accuracy: 0.169
Epoch 2: loss=28835.94555
Test accuracy: 0.211
Train accuracy: 0.223
Epoch 3: loss=5692.08440
Test accuracy: 0.195
Train accuracy: 0.261
Epoch 4: loss=14.53138
Test accuracy: 0.184
Train accuracy: 0.261
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=9.20423
Test accuracy: 0.199
Train accuracy: 0.196
Epoch 2: loss=3.41934
Test accuracy: 0.227
Train accuracy: 0.293
Epoch 3: loss=2.39396
Test accuracy: 0.242
Train accuracy: 0.397
Epoch 4: loss=1.81781
Test accuracy: 0.230
Train accuracy: 0.476
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=582.11707
Test accuracy: 0.109
Train accuracy: 0.106
Epoch 2: loss=10644.02584
Test accuracy: 0.152
Train accuracy: 0.123
Epoch 3: loss=7169152.01771
Test accuracy: 0.156
Train accuracy: 0.160
Epoch 4: loss=15.04353
Test accuracy: 0.148
Train accuracy: 0.143
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Traceback (most recent call last):
  File "swep_regDVS.py", line 403, in <module>
    loss_hist, test_acc, train_acc, best = train(x_train, y_train, lr = quantization.global_lr, nb_epochs = 4)
  File "swep_regDVS.py", line 348, in train
    loss_val.backward()
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/afs/crc.nd.edu/user/c/cschaef6/.local/lib/python3.7/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 10.73 GiB total capacity; 8.61 GiB already allocated; 173.56 MiB free; 1.03 GiB cached)
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=17.96129
Test accuracy: 0.070
Train accuracy: 0.074
Epoch 2: loss=7.14475
Test accuracy: 0.086
Train accuracy: 0.134
Epoch 3: loss=4.10250
Test accuracy: 0.148
Train accuracy: 0.204
Epoch 4: loss=3.21531
Test accuracy: 0.199
Train accuracy: 0.273
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=266.70167
Test accuracy: 0.137
Train accuracy: 0.122
Epoch 2: loss=225623221993477.59375
Test accuracy: 0.199
Train accuracy: 0.179
Epoch 3: loss=2137385395092.34668
Test accuracy: 0.160
Train accuracy: 0.224
Epoch 4: loss=1705699255575646.75000
Test accuracy: 0.160
Train accuracy: 0.241
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=77.67369
Test accuracy: 0.215
Train accuracy: 0.162
Epoch 2: loss=9.68486
Test accuracy: 0.203
Train accuracy: 0.239
Epoch 3: loss=36.41156
Test accuracy: 0.250
Train accuracy: 0.264
Epoch 4: loss=6.06399
Test accuracy: 0.246
Train accuracy: 0.281
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=826.55093
Test accuracy: 0.184
Train accuracy: 0.169
Epoch 2: loss=58607.74286
Test accuracy: 0.207
Train accuracy: 0.215
Epoch 3: loss=6.75660
Test accuracy: 0.164
Train accuracy: 0.231
Epoch 4: loss=7.91669
Test accuracy: 0.137
Train accuracy: 0.194
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=93.60308
Test accuracy: 0.102
Train accuracy: 0.096
Epoch 2: loss=3124.93395
Test accuracy: 0.145
Train accuracy: 0.220
Epoch 3: loss=4.57007
Test accuracy: 0.113
Train accuracy: 0.181
Epoch 4: loss=4.47617
Test accuracy: 0.094
Train accuracy: 0.172
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=40.32775
Test accuracy: 0.176
Train accuracy: 0.142
Epoch 2: loss=8.27074
Test accuracy: 0.172
Train accuracy: 0.166
Epoch 3: loss=20.65656
Test accuracy: 0.152
Train accuracy: 0.199
Epoch 4: loss=4.77326
Test accuracy: 0.191
Train accuracy: 0.219
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=9.30559
Test accuracy: 0.234
Train accuracy: 0.239
Epoch 2: loss=133325.51764
Test accuracy: 0.207
Train accuracy: 0.359
Epoch 3: loss=2.76769
Test accuracy: 0.230
Train accuracy: 0.426
Epoch 4: loss=2.61864
Test accuracy: 0.211
Train accuracy: 0.469
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=73.24286
Test accuracy: 0.094
Train accuracy: 0.102
Epoch 2: loss=12730.95374
Test accuracy: 0.090
Train accuracy: 0.119
Epoch 3: loss=40514.11401
Test accuracy: 0.141
Train accuracy: 0.135
Epoch 4: loss=11474153.50443
Test accuracy: 0.129
Train accuracy: 0.122
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=310.64110
Test accuracy: 0.230
Train accuracy: 0.223
Epoch 2: loss=3520682.77742
Test accuracy: 0.211
Train accuracy: 0.299
Epoch 3: loss=8.99240
Test accuracy: 0.086
Train accuracy: 0.218
Epoch 4: loss=10.95238
Test accuracy: 0.102
Train accuracy: 0.190
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=14.65675
Test accuracy: 0.105
Train accuracy: 0.116
Epoch 2: loss=5.58491
Test accuracy: 0.105
Train accuracy: 0.131
Epoch 3: loss=3.57723
Test accuracy: 0.125
Train accuracy: 0.201
Epoch 4: loss=2.97123
Test accuracy: 0.164
Train accuracy: 0.269
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=9.39556
Test accuracy: 0.242
Train accuracy: 0.241
Epoch 2: loss=3.14628
Test accuracy: 0.242
Train accuracy: 0.382
Epoch 3: loss=2.28313
Test accuracy: 0.281
Train accuracy: 0.478
Epoch 4: loss=1.83829
Test accuracy: 0.285
Train accuracy: 0.589
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 4000, 'nb_steps': 150, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=414337224980323.62500
Test accuracy: 0.082
Train accuracy: 0.084
Epoch 2: loss=122.15340
Test accuracy: 0.102
Train accuracy: 0.095
Epoch 3: loss=1353.52529
Test accuracy: 0.109
Train accuracy: 0.118
Epoch 4: loss=165.82002
Test accuracy: 0.121
Train accuracy: 0.151
Loading python/3.7.3
  Loading requirement: tcl/8.6.8 gcc/8.3.0
No handles with labels found to put in legend.
init done
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=4.53308
Test accuracy: 0.258
Train accuracy: 0.248
Epoch 2: loss=2.07731
Test accuracy: 0.285
Train accuracy: 0.360
Epoch 3: loss=1.60825
Test accuracy: 0.285
Train accuracy: 0.469
Epoch 4: loss=1.33500
Test accuracy: 0.336
Train accuracy: 0.585
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=4.45176
Test accuracy: 0.148
Train accuracy: 0.164
Epoch 2: loss=2.03638
Test accuracy: 0.195
Train accuracy: 0.297
Epoch 3: loss=1.58552
Test accuracy: 0.285
Train accuracy: 0.455
Epoch 4: loss=1.36209
Test accuracy: 0.316
Train accuracy: 0.572
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=2.81413
Test accuracy: 0.309
Train accuracy: 0.272
Epoch 2: loss=1.50674
Test accuracy: 0.312
Train accuracy: 0.528
Epoch 3: loss=1.20130
Test accuracy: 0.363
Train accuracy: 0.662
Epoch 4: loss=0.89335
Test accuracy: 0.352
Train accuracy: 0.785
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=4.54533
Test accuracy: 0.316
Train accuracy: 0.270
Epoch 2: loss=2.07683
Test accuracy: 0.336
Train accuracy: 0.390
Epoch 3: loss=1.67829
Test accuracy: 0.359
Train accuracy: 0.494
Epoch 4: loss=1.45569
Test accuracy: 0.359
Train accuracy: 0.601
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=4.09559
Test accuracy: 0.180
Train accuracy: 0.201
Epoch 2: loss=2.14545
Test accuracy: 0.219
Train accuracy: 0.411
Epoch 3: loss=1.80718
Test accuracy: 0.230
Train accuracy: 0.548
Epoch 4: loss=1.59934
Test accuracy: 0.289
Train accuracy: 0.634
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=5.52051
Test accuracy: 0.250
Train accuracy: 0.253
Epoch 2: loss=3.16763
Test accuracy: 0.293
Train accuracy: 0.391
Epoch 3: loss=2.84364
Test accuracy: 0.297
Train accuracy: 0.507
Epoch 4: loss=2.61842
Test accuracy: 0.316
Train accuracy: 0.579
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=8.74746
Test accuracy: 0.164
Train accuracy: 0.188
Epoch 2: loss=6.42074
Test accuracy: 0.195
Train accuracy: 0.345
Epoch 3: loss=6.05337
Test accuracy: 0.246
Train accuracy: 0.480
Epoch 4: loss=5.77266
Test accuracy: 0.289
Train accuracy: 0.591
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=19.14383
Test accuracy: 0.297
Train accuracy: 0.257
Epoch 2: loss=17.06935
Test accuracy: 0.285
Train accuracy: 0.431
Epoch 3: loss=16.36819
Test accuracy: 0.316
Train accuracy: 0.543
Epoch 4: loss=15.71312
Test accuracy: 0.352
Train accuracy: 0.661
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=57.83281
Test accuracy: 0.223
Train accuracy: 0.223
Epoch 2: loss=53.83992
Test accuracy: 0.285
Train accuracy: 0.423
Epoch 3: loss=51.51699
Test accuracy: 0.328
Train accuracy: 0.523
Epoch 4: loss=49.25504
Test accuracy: 0.340
Train accuracy: 0.595
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=189.88617
Test accuracy: 0.266
Train accuracy: 0.247
Epoch 2: loss=179.02343
Test accuracy: 0.301
Train accuracy: 0.358
Epoch 3: loss=170.22801
Test accuracy: 0.324
Train accuracy: 0.436
Epoch 4: loss=161.80196
Test accuracy: 0.371
Train accuracy: 0.505
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=640.44234
Test accuracy: 0.242
Train accuracy: 0.246
Epoch 2: loss=606.16382
Test accuracy: 0.270
Train accuracy: 0.334
Epoch 3: loss=575.03212
Test accuracy: 0.281
Train accuracy: 0.374
Epoch 4: loss=545.11989
Test accuracy: 0.254
Train accuracy: 0.408
{'quantization.global_wb': 8, 'inp_mult': 110, 'nb_hidden': 2500, 'nb_steps': 100, 'batch_size': 64, 'quantization.global_lr': 1.5e-05, 'reg_size': 0.001, 'mult_eq': 0.12, 'class_method': 'integrate', 'weight_sum': [1.12, 0.982]}
Epoch 1: loss=2186.76152
Test accuracy: 0.219
Train accuracy: 0.200
Epoch 2: loss=2072.23881
Test accuracy: 0.219
Train accuracy: 0.314
Epoch 3: loss=1963.83779
Test accuracy: 0.266
Train accuracy: 0.326
Epoch 4: loss=1860.14349
Test accuracy: 0.285
Train accuracy: 0.348
